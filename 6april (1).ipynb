{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbe866e-5e06-4bec-9a29-68f21bcc8342",
   "metadata": {},
   "source": [
    "# #Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9928b-ac87-4f37-a495-e548f11d3b1b",
   "metadata": {},
   "source": [
    "The mathematical formulation for a linear Support Vector Machine (SVM) involves finding a hyperplane that best separates the data into different classes. The objective is to maximize the margin between the classes while minimizing the classification error. Here's the basic formulation for a linear SVM:\n",
    "\n",
    "Given a set of training data points \\((x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\), where \\(x_i\\) represents the feature vectors and \\(y_i\\) represents the class labels (+1 or -1), the linear SVM aims to find the hyperplane \\(w \\cdot x + b = 0\\) that separates the data, where \\(w\\) is the weight vector and \\(b\\) is the bias term.\n",
    "\n",
    "The decision function for classifying a new data point \\(x\\) is given by:\n",
    "\n",
    "\\[f(x) = w \\cdot x + b\\]\n",
    "\n",
    "The goal of SVM is to find the optimal \\(w\\) and \\(b\\) such that:\n",
    "1. The margin (distance between the hyperplane and the nearest data point of any class) is maximized.\n",
    "2. Data points are correctly classified, i.e., \\(y_i (w \\cdot x_i + b) \\geq 1\\) for support vectors (data points near the margin) of both classes.\n",
    "\n",
    "Mathematically, the optimization problem for a linear SVM can be formulated as:\n",
    "\n",
    "\\[\\text{Minimize } \\frac{1}{2} ||w||^2 \\text{ subject to } y_i (w \\cdot x_i + b) \\geq 1 \\text{ for all } i\\]\n",
    "\n",
    "Where:\n",
    "- \\(||w||\\) represents the Euclidean norm of the weight vector.\n",
    "- \\(y_i\\) is the class label (+1 or -1) of data point \\(x_i\\).\n",
    "- The constraint ensures that data points are correctly classified and lie outside the margin.\n",
    "\n",
    "Solving this optimization problem results in finding the optimal \\(w\\) and \\(b\\) that define the separating hyperplane.\n",
    "\n",
    "Please note that SVM formulations can vary based on factors such as handling soft-margin SVM (allowing some misclassification) and adding regularization terms to handle cases with overlapping data. The above formulation is for a linear, hard-margin SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15107673-118f-47b8-a3ad-d71b8235048c",
   "metadata": {},
   "source": [
    " # #Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c1a77-956d-4334-9b22-95c073a83d89",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is to find the parameters that define a hyperplane that maximizes the margin between classes while correctly classifying the data points. In a linear SVM, the primary goal is to find a hyperplane that best separates the data into different classes while minimizing classification errors. The objective function involves finding the optimal weights (\\(w\\)) and bias (\\(b\\)) terms for the hyperplane.\n",
    "\n",
    "Mathematically, the objective function of a linear SVM can be expressed as follows:\n",
    "\n",
    "\\[\\text{Minimize } \\frac{1}{2} ||w||^2\\]\n",
    "\n",
    "subject to:\n",
    "\n",
    "\\[y_i (w \\cdot x_i + b) \\geq 1 \\text{ for all } i\\]\n",
    "\n",
    "Where:\n",
    "- \\(w\\) represents the weight vector that defines the direction of the hyperplane.\n",
    "- \\(b\\) is the bias term that determines the offset of the hyperplane from the origin.\n",
    "- \\(x_i\\) is the feature vector of the \\(i\\)th data point.\n",
    "- \\(y_i\\) is the class label (+1 or -1) of the \\(i\\)th data point.\n",
    "- \\(||w||\\) represents the Euclidean norm of the weight vector.\n",
    "\n",
    "The objective function aims to minimize the \\(L2\\) norm of the weight vector (\\(||w||^2\\)), which has the effect of maximizing the margin between the classes. The constraints \\(y_i (w \\cdot x_i + b) \\geq 1\\) ensure that data points of both classes are correctly classified and lie outside the margin.\n",
    "\n",
    "In the case of linear SVM, this is referred to as a hard-margin SVM, where no misclassifications are allowed. Soft-margin SVM allows for some misclassifications and introduces slack variables to handle cases where the data isn't perfectly separable.\n",
    "\n",
    "The optimization problem associated with the objective function seeks to strike a balance between maximizing the margin and minimizing the classification errors, leading to a hyperplane that generalizes well to new data points and achieves good separation between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ad748-d6de-4458-ace4-7ded9c4ce658",
   "metadata": {},
   "source": [
    "# #Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40a85c-bd5a-4484-a9af-0f0a3a7a1553",
   "metadata": {},
   "source": [
    "The kernel trick is a powerful concept in Support Vector Machines (SVMs) that allows the SVM algorithm to operate in a higher-dimensional space without explicitly computing the coordinates of data points in that space. This trick is particularly useful when dealing with non-linearly separable data. It enables SVMs to effectively handle complex relationships between features and find non-linear decision boundaries.\n",
    "\n",
    "Here's how the kernel trick works:\n",
    "\n",
    "1. **Linearly Inseparable Data:**\n",
    "   - In some cases, data points in their original feature space cannot be separated by a linear hyperplane. A linear SVM wouldn't be able to effectively classify such data.\n",
    "\n",
    "2. **Mapping to Higher Dimension:**\n",
    "   - The kernel trick involves mapping the original feature space into a higher-dimensional space using a mathematical function called a \"kernel.\"\n",
    "   - The kernel function calculates the dot product between two data points in the higher-dimensional space without explicitly calculating the coordinates of those points.\n",
    "\n",
    "3. **Kernel Functions:**\n",
    "   - Commonly used kernel functions include:\n",
    "     - Linear Kernel: \\(K(x, x') = x \\cdot x'\\)\n",
    "     - Polynomial Kernel: \\(K(x, x') = (x \\cdot x' + c)^d\\)\n",
    "     - Gaussian (Radial Basis Function) Kernel: \\(K(x, x') = e^{-\\gamma ||x - x'||^2}\\)\n",
    "     - Sigmoid Kernel: \\(K(x, x') = \\tanh(\\alpha x \\cdot x' + c)\\)\n",
    "\n",
    "4. **Benefits:**\n",
    "   - The kernel trick effectively transforms the data into a higher-dimensional space, where linear separation might be possible.\n",
    "   - It avoids the computational burden of explicitly transforming data points and calculating their coordinates in the higher-dimensional space.\n",
    "   - SVMs can find a non-linear decision boundary in the higher-dimensional space, effectively classifying complex data that would be difficult for linear classifiers.\n",
    "\n",
    "5. **Kernel Matrix:**\n",
    "   - The kernel trick leads to the creation of a \"kernel matrix,\" which contains the pairwise similarities (computed using the kernel function) between all data points.\n",
    "   - This kernel matrix is used in the SVM optimization process to determine the optimal hyperplane that best separates the classes in the higher-dimensional space.\n",
    "\n",
    "The kernel trick significantly enhances the flexibility of SVMs, enabling them to handle non-linear data without explicitly dealing with the complexities of working in higher-dimensional spaces. It's an elegant way to transform the problem while retaining the efficiency and effectiveness of SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee34797-d14c-4916-8906-220278e5c43c",
   "metadata": {},
   "source": [
    " # #Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44dda6-414e-44bb-9989-16dd59ef8490",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVMs), support vectors play a critical role in defining the decision boundary and determining the margin between classes. Support vectors are the data points that lie closest to the decision boundary (hyperplane). These points directly influence the position and orientation of the hyperplane and are crucial for the SVM's performance. Let's explore the role of support vectors with an example:\n",
    "\n",
    "**Example: Binary Classification of Iris Flowers**\n",
    "\n",
    "Suppose we're working with the classic Iris dataset, which contains features of different iris flowers. We want to classify two types of iris flowers: Setosa and Versicolor. We'll focus on just two features for simplicity.\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - We select two features: sepal length and sepal width.\n",
    "   - We label Setosa flowers as class +1 and Versicolor flowers as class -1.\n",
    "\n",
    "2. **Training SVM:**\n",
    "   - After training a linear SVM, it finds the optimal hyperplane that separates the Setosa and Versicolor flowers.\n",
    "\n",
    "3. **Support Vectors:**\n",
    "   - The support vectors are the data points that lie closest to the decision boundary (hyperplane).\n",
    "   - In this case, these are the data points that are on or near the margin or the points that are misclassified.\n",
    "\n",
    "4. **Role of Support Vectors:**\n",
    "   - The support vectors directly influence the position and orientation of the hyperplane.\n",
    "   - The margin of the SVM is determined by the distance between the hyperplane and the support vectors. The larger the margin, the better the generalization to new data.\n",
    "   - The support vectors essentially \"support\" the decision boundary by helping to define it.\n",
    "\n",
    "5. **Influence on Margin:**\n",
    "   - The distance between the support vectors and the hyperplane is crucial. These support vectors define the margin.\n",
    "   - If a support vector is moved further away from the hyperplane, it would impact the margin and potentially the classification of other points.\n",
    "\n",
    "6. **Robustness:**\n",
    "   - The support vectors are critical for the SVM's robustness and ability to generalize well to new, unseen data.\n",
    "   - Even if other data points were removed or changed, as long as the support vectors remain in their positions, the decision boundary is likely to stay relatively stable.\n",
    "\n",
    "In summary, support vectors are the key data points that directly influence the decision boundary and the margin in an SVM. They determine the placement and orientation of the hyperplane. By focusing on the most relevant data points, SVMs are able to create a decision boundary that best separates different classes while maintaining robustness and generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63895c-81fe-4a61-9dd7-b55732c2d7a7",
   "metadata": {},
   "source": [
    " # #Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0577c-3d4c-414b-8a4b-f4cbee020aff",
   "metadata": {},
   "source": [
    "Certainly, I'll illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM) using examples and graphs.\n",
    "\n",
    "**Example Scenario: 2D Data Classification**\n",
    "\n",
    "Consider a scenario where we have a dataset with two features, and we want to classify two classes: circles and squares.\n",
    "\n",
    "**1. Hyperplane:**\n",
    "A hyperplane is a decision boundary that separates the two classes. In a 2D space, a hyperplane is a straight line.\n",
    "\n",
    "**2. Marginal Plane:**\n",
    "The marginal plane consists of two parallel lines that run along the margins (edges) of the \"corridor\" formed by support vectors. It helps define the width of the margin in a soft-margin SVM.\n",
    "\n",
    "**3. Hard Margin:**\n",
    "A hard-margin SVM aims to find a hyperplane that completely separates the classes without any misclassifications. It doesn't tolerate any points inside the margin.\n",
    "\n",
    "**4. Soft Margin:**\n",
    "A soft-margin SVM allows for a certain number of misclassifications and points to be within the margin. It introduces a trade-off between maximizing the margin and minimizing the misclassifications.\n",
    "\n",
    "Let's visualize these concepts:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(20, 2)\n",
    "y = np.array([1] * 10 + [-1] * 10)\n",
    "\n",
    "# Create a scatter plot of the data\n",
    "plt.scatter(X[:10, 0], X[:10, 1], marker='o', label='Class 1 (Circles)')\n",
    "plt.scatter(X[10:, 0], X[10:, 1], marker='s', label='Class -1 (Squares)')\n",
    "\n",
    "# Define the hyperplane\n",
    "w = np.array([0.5, -0.5])  # Normal vector to the hyperplane\n",
    "b = 0  # Bias term\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = -(w[0] / w[1]) * x_range - (b / w[1])\n",
    "plt.plot(x_range, y_range, 'k-', label='Hyperplane')\n",
    "\n",
    "# Define the marginal planes for soft margin\n",
    "margin = 0.5\n",
    "upper_margin = y_range + margin\n",
    "lower_margin = y_range - margin\n",
    "plt.plot(x_range, upper_margin, 'r--', label='Upper Marginal Plane')\n",
    "plt.plot(x_range, lower_margin, 'b--', label='Lower Marginal Plane')\n",
    "\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM Concepts: Hyperplane, Marginal Planes, Soft Margin')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "In the above graph:\n",
    "- The solid black line represents the hyperplane that separates the two classes.\n",
    "- The red dashed line represents the upper marginal plane.\n",
    "- The blue dashed line represents the lower marginal plane.\n",
    "- The shaded corridor between the marginal planes is the soft margin region.\n",
    "\n",
    "This example visually demonstrates the concepts of the hyperplane, marginal planes, soft margin, and hard margin in an SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761faa9-8eb7-4ad8-aec8-8fad3e37793a",
   "metadata": {},
   "source": [
    "# #Q6. SVM Implementation through Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f99c3a-2986-4f64-9a6f-6aadc2f66aad",
   "metadata": {},
   "source": [
    "Sure, I can guide you through implementing a linear SVM classifier from scratch using Python and comparing its performance with the scikit-learn implementation. Let's get started!\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implement a simple linear SVM from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, n_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = np.random.randn(X.shape[1])\n",
    "        self.b = 0\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            scores = np.dot(X, self.w) + self.b\n",
    "            margins = y * scores\n",
    "            misclassified_indices = np.where(margins < 1)[0]\n",
    "            dw = -np.dot(X[misclassified_indices].T, y[misclassified_indices])\n",
    "            db = -np.sum(y[misclassified_indices])\n",
    "            self.w -= self.learning_rate * dw\n",
    "            self.b -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_scratch = LinearSVM()\n",
    "svm_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred_scratch = svm_scratch.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_scratch = accuracy_score(y_test, y_pred_scratch)\n",
    "print(f\"Accuracy (Scratch Implementation): {accuracy_scratch:.2f}\")\n",
    "\n",
    "# Compare with scikit-learn SVM implementation\n",
    "from sklearn.svm import SVC\n",
    "svm_sklearn = SVC(kernel='linear')\n",
    "svm_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy (scikit-learn Implementation): {accuracy_sklearn:.2f}\")\n",
    "\n",
    "# Plot the decision boundary using two features\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for scratch implementation\n",
    "plot_decision_boundary(svm_scratch, X_train[:, :2], y_train)\n",
    "\n",
    "# Plot decision boundary for scikit-learn implementation\n",
    "plot_decision_boundary(svm_sklearn, X_train[:, :2], y_train)\n",
    "```\n",
    "\n",
    "In this code, we first implement a simple linear SVM classifier from scratch. Then, we compare its performance with the scikit-learn SVM implementation using the Iris dataset. The code computes and prints the accuracy for both implementations and plots the decision boundary using two features. You can adjust the hyperparameters and try different values to observe their effects on the accuracy and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa26ab0-83f0-4f4b-8344-59f9142a5838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
